# 分布式锁

## 基于Zookeeper

zookeeper也被成为分布式协调服务，利用它可以创建临时有序的结点，Watch，来解决分布式锁的问题。

- 临时（ephemeral）的结点会随着当前归属的Session会话的断开，而删除，这一特性就有效的解决了当某一线程持有锁，突然该线程就挂了，而锁又没有释放导致了其它线程无法获得锁的问题。
- 有序（sequential）结点，每个被创建的结点都是有序号的，序号只会递增不会减小，这就可以解决在多线程情况下，同时请求获取锁时，是哪个线程先创建的锁结点。
- Watch，监控指定结点，当结点发生任何变化时，监控该节点的线程立即知道状态的变化，还解决了延迟问题。

> 总结：
>
> 利用以上特性，多个线程创建了结点后，除了序号排在第一位的结点线程获得锁的执行权，其他的线程都会排队等待，同时等待的线程都会监控自己前一个结点目录，这样当锁目录被释放删除后，监控锁的那个线程就会获得锁。

```java
public class LockCallback implements Watcher, AsyncCallback.StringCallback, AsyncCallback.Children2Callback, AsyncCallback.StatCallback {

    ZooKeeper zk;
    String threadName;
    String pathName;
    CountDownLatch latch = new CountDownLatch(1);

    public void setZk(ZooKeeper zk) {
        this.zk = zk;
    }

    public void setThreadName(String threadName) {
        this.threadName = threadName;
    }
	//尝试获取锁
    public void tryLock() {
        zk.create("/lock", threadName.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL, this, " 创建锁结点");
        try {
            //直到获取到锁才放行
            latch.await();
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }
	//释放锁
    public void unLock(){
        try {
            zk.delete(pathName,-1 );
        } catch (InterruptedException e) {
            e.printStackTrace();
        } catch (KeeperException e) {
            e.printStackTrace();
        }
    }

    @Override
    public void process(WatchedEvent event) {

        switch (event.getType()) {
            case None:
                break;
            case NodeCreated:
                break;
            case NodeDeleted:
                System.out.println("锁被释放了");
                zk.getChildren("/", false, this, "获得/路径下的所有锁目录");
                break;
            case NodeDataChanged:
                break;
            case NodeChildrenChanged:
                break;
        }
    }

    @Override
    public void processResult(int rc, String path, Object ctx, String name) {
        if (name != null && name != "") {
            System.out.println(threadName + " " + ctx);
            pathName = name;
            zk.getChildren("/", false, this, "获得/路径下的所有锁目录");
        }
    }

    @Override
    public void processResult(int rc, String path, Object ctx, List<String> children, Stat stat) {
        //对所有结点排序
        Collections.sort(children);
        //获得自己创建的结点的排在第几个
        int index = children.indexOf(pathName.substring(1));
        System.out.println(threadName + " " + ctx);
		//如果自己创建的结点是第一个，获得锁
        if (index == 0) {
            System.out.println(threadName + " is first!");
            latch.countDown();//该线程成功获取到锁，就放行
        } else {//如果不是，监听前一个结点
            zk.exists("/" + children.get(index - 1), this, this, "监听的前一个锁目录状态发生变换");
        }
    }

    @Override
    public void processResult(int rc, String path, Object ctx, Stat stat) {
//        System.out.println(threadName + " " + ctx + " stat:" + stat);
    }
}
```

## 基于Redis

使用redis实现分布式锁，要知道以下几点：

1. redis是单进程单线程
2. 缓存要设置有效期，有效期到，删除数据
3. setnx操作，当key不存在时，才设置，存在时不做任何操作

**单节点redis**

加锁操作：set id value NX PX 3000

这条命令如果执行成功，则客户端成功获取到锁，接下来就可以访问共享资源；执行失败则获取锁失败。

**释放锁**

要判断释放的是不是自己的锁

**关注点**

1. id：就是我们的key，要锁定的目标

2. value：代表获得锁的对象值

3. NX：只有当id不存在时才能被set成功。这就保证了只有第一个请求的客户端才能获得锁

4. PX 3000：表示这个key3秒后自动过期

5. 重点：这个锁必须设置一个过期时间，否则，如果获得锁的客户端崩溃了或者是无法跟redis通信，那么这把锁就会一直被持有着，而其他客户端无法获得锁

6. 这一系列的操作不能分割，要保证原子性

   ```sh
   ## 错误展示
   SETNX id value #设置锁
   EXPIRE orderId 30 #给锁设置有效时间
   #如果客户端第一步操作成功，然后就崩溃了，就没机会执行第二步，然后就造成无法释放锁，死锁
   ##必需一步到位
   set id value NX PX 3000
   ```

7. 释放锁要判断是不是自己的锁

   ```
   客户端1获得锁后，但是业务执行时间太长，锁有效时间过期被自动释放
   客户端2获得了锁，然后执行代码
   客户端1业务执行结束，释放锁操作不判断锁的value是不是自己，就会释放客户端2的锁
   ```

8. redis故障问题

   redis配置主从，当master不可用时，系统切换到slave时，由于redis主从复制是异步的，这可能丧失锁的安全性。

   ```
   客户端1在master上获得了锁
   master宕机了，但是锁还没有同步到slave上
   这时slave升级为master
   客户端2又从新master上获得同一资源的锁
   ```

   客户端1和客户端2同时持有同一资源的锁。锁的安全性被打破

   > #### 注：以上都是使用StringRedisTemplate获取锁需要注意的点，同时还需要自己另起一个线程延长key的有效期时间
   >
   > ```java
   > @Autowired
   > 	private StringRedisTemplate stringRedisTemplate;
   > 
   > 	@Override
   > 	public ResponseResult grabOrder(int orderId, int driverId) {
   > 		System.out.println("司机"+driverId+"请求过来");
   > 		
   > 		// 生成key
   > 		String keyString = "Order_" + orderId;
   > 
   > 		// 注：这里要加上超时时间，而且必须在一行上设置超时时间，
   > 		// 原因：防止代码还没有执行到锁释放，服务器重启或服务器挂了，那么锁没有超时时间就导致其他人获取不到锁
   > 		Boolean lockStatus = stringRedisTemplate.opsForValue().setIfAbsent(keyString.intern(), driverId + "", 30,TimeUnit.SECONDS);
   > 		System.out.println(driverId+" :"+lockStatus);
   > 		if (!lockStatus) {
   > 			return null;
   > 		}
   > 		try {
   > 			System.out.println(driverId+" 执行抢单逻辑");
   > 		} finally {
   > 			//注：这里必须加上判断，否则可能会释放别人的锁
   > 			//原因：如果A的代码执行时间超过了有效期，锁被被释放了，然后B获取到了锁，A执行到这里需要判断当前这个锁是不是自己的
   > 			if ((driverId + "").equals(stringRedisTemplate.opsForValue().get(keyString.intern()))) {
   > 				stringRedisTemplate.delete(keyString.intern());
   > 			}
   > 		}
   > 		return null;
   > 	}
   > ```

9. 程序没有执行完，是不能让key过期的

   使用RedissonClient操作，

   ```java
   @Autowired
   	private RedissonClient redissonClient;
   	
   	@Override
   	public ResponseResult grabOrder(int orderId, int driverId) {
   		String lockString="OrderId_"+orderId;
   		RLock lock = redissonClient.getLock(lockString.intern());
   		
   		try {
   			lock.lock();//会自动给key延长有效时间
   			System.out.println("司机:"+driverId+" 执行抢单逻辑");
   			Thread.sleep(2000);
   		} catch (InterruptedException e) {
   			e.printStackTrace();
   		} finally {
   			lock.unlock();
   		}
   		return null;
   	}
   ```

   > ### 注：这还是会存在Redis故障问题

**为了解决以上问题**

提出了Redis红锁（RedLock）

基于N个完全独立的Redis结点

运行Redlock算法的客户端依次执行下面各个步骤，来完成 获取锁 的操作：

1. 获取当前时间（毫秒数）。
2. 按顺序依次向N个Redis节点执行 **获取锁** 的操作。这个获取操作跟前面基于单Redis节点的 **获取锁** 的过程相同，包含value driverId ，也包含过期时间(比如 `PX 30000` ，即锁的有效时间)。为了保证在某个Redis节点不可用的时候算法能够继续运行，这个 **获取锁** 的操作还有一个超时时间(time out)，它要远小于锁的有效时间（几十毫秒量级）。客户端在向某个Redis节点获取锁失败以后，应该立即尝试下一个Redis节点。这里的失败，应该包含任何类型的失败，比如该Redis节点不可用，或者该Redis节点上的锁已经被其它客户端持有（注：Redlock原文中这里只提到了Redis节点不可用的情况，但也应该包含其它的失败情况）。
3. 计算整个获取锁的过程总共消耗了多长时间，计算方法是用当前时间减去第1步记录的时间。如果客户端从大多数Redis节点（>= N/2+1）成功获取到了锁，并且获取锁总共消耗的时间没有超过锁的有效时间(lock validity time)，那么这时客户端才认为最终获取锁成功；否则，认为最终获取锁失败。
4. 如果最终获取锁成功了，那么这个锁的有效时间应该重新计算，它等于最初的锁的有效时间减去第3步计算出来的获取锁消耗的时间。
5. 如果最终获取锁失败了（可能由于获取到锁的Redis节点个数少于N/2+1，或者整个获取锁的过程消耗的时间超过了锁的最初有效时间），那么客户端应该立即向所有Redis节点发起 **释放锁** 的操作（即前面介绍的Redis Lua脚本）。

当然，上面描述的只是 获取锁 的过程，而 释放锁 的过程比较简单：客户端向所有Redis节点发起 释放锁 的操作，不管这些节点当时在获取锁的时候成功与否。

   	

问题：

由于N个Redis节点中的大多数能正常工作就能保证Redlock正常工作，因此理论上它的可用性更高。我们前面讨论的单Redis节点的分布式锁在failover的时候锁失效的问题，在Redlock中不存在了，但如果有节点发生崩溃重启，还是会对锁的安全性有影响的。具体的影响程度跟Redis对数据的持久化程度有关。

假设一共有5个Redis节点：A, B, C, D, E。设想发生了如下的事件序列：

1. 客户端1成功锁住了A, B, C， **获取锁** 成功（但D和E没有锁住）。
2. 节点C崩溃重启了，但客户端1在C上加的锁没有持久化下来，丢失了。
3. 节点C重启后，客户端2锁住了C, D, E， **获取锁** 成功。

这样，客户端1和客户端2同时获得了锁（针对同一资源）。

在默认情况下，Redis的AOF持久化方式是每秒写一次磁盘（即执行fsync），因此最坏情况下可能丢失1秒的数据。为了尽可能不丢数据，Redis允许设置成每次修改数据都进行fsync，但这会降低性能。当然，即使执行了fsync也仍然有可能丢失数据（这取决于系统而不是Redis的实现）。所以，上面分析的由于节点重启引发的锁失效问题，总是有可能出现的。为了应对这一问题，antirez又提出了 延迟重启 (delayed restarts)的概念。也就是说，一个节点崩溃后，先不立即重启它，而是等待一段时间再重启，这段时间应该大于锁的有效时间(lock validity time)。这样的话，这个节点在重启前所参与的锁都会过期，它在重启后就不会对现有的锁造成影响。

关于Redlock还有一点细节值得拿出来分析一下：在最后 释放锁 的时候，antirez在算法描述中特别强调，客户端应该向所有Redis节点发起 释放锁 的操作。也就是说，即使当时向某个节点获取锁没有成功，在释放锁的时候也不应该漏掉这个节点。这是为什么呢？设想这样一种情况，客户端发给某个Redis节点的 获取锁 的请求成功到达了该Redis节点，这个节点也成功执行了 `SET`操作，但是它返回给客户端的响应包却丢失了。这在客户端看来，获取锁的请求由于超时而失败了，但在Redis这边看来，加锁已经成功了。因此，释放锁的时候，客户端也应该对当时获取锁失败的那些Redis节点同样发起请求。实际上，这种情况在异步通信模型中是有可能发生的：客户端向服务器通信是正常的，但反方向却是有问题的。

# 分布式事务

目标：解决多个独立事务的一致性问题

问题：一个功能，横跨多个微服务，由于每个微服务不在一个库中，没法使用数据库事务来保证事务。

**解决方案**

二阶段提交协议

基于XA协议，采取强一致性，遵从ACID;

> XA:主要定义了全局事务管理器（Transaction Manager）和局部资源管理器（Resource Manager）之间的接口。XA接口是双向的系统接口，在事务管理器及多个资源管理器之间形成同心桥梁。也就是说，在基于XA的一个事务中，我们可以对多个资源进行事务管理。

过程：

```
1.请求阶段
在请求阶段，协调者将通知事务参与者准备提交或取消事务，然后进入表决过程。
在表决过程中，参与者将告知协调者自己的决策：同意（事务参与者本地作业执行成功）或取消（本地作业执行错误）

2.提交阶段
在该阶段，协调者将基于第一阶段的投票结果进行决策：提交或取消。
当且仅当所有的参与同意提交事务，协调者才会通知所有的参与者提交事务，否则协调者将通知所有的参与者回滚取消事务。
参与者在接受到协调者发来的消息后执行响应操作。
```

缺点很明显：

- 单点故障：事务的法发起，提交，还是取消，都是由老大协调者管理的，只要协调者挂了，那就都挂了

- 同步阻塞：参与者在没接收协调者的命令，提交还是取消的事务命令的时候，就是锁定当前资源，整个过程都是阻塞的
- 数据不一致：协调者向参与者发出命令后，由于网络的问题，造成部分参与者没有收到命令，就会出现数据不一致

无法解决的问题

当协调者发出commit命令后挂了，而唯一接受到消息的参与者同时也挂了。

那么即使有新的协调者，这条事务的状态也不知道，没人知道事务是否被提交。



三阶段提交协议

采取强一致性，遵守ACID

在二阶段上增加：超时和预提交机制

三个主阶段：canCommit、preCommit、doCommit这三个阶段

流程：

```
1.canCommit
协调者向参与者发送Commit请求，参与者如果可以提交就返回yes，否则返回No
2.PreCommit
协调者根据上一阶段的反应情况决定是否可以继续事务的PreCommit操作
根据响应情况，
A：如果反馈都是yes，那么进行事务的预执行。协调者发送预提交请求，参与者如果成功执行了事务操作，就返回ACK，等待最终指令
B：如果返回有NO，或者等待超时之后，就中断事务
协调者发送中断请求
3.DoCommit阶段
该阶段进行事务提交，
协调者发送提交请求，参与者收到doCommit命令后，执行书屋提交，并在完成事务提交后释放所有事务资源。向协调者返回ACK响应
```

二和三的区别

加大了询问，增大成功概率。

在协调者和参与者都设置了超时机制（二中，只有协调者有超时机制，如果在一定时间没有收到参与者消息则默认失败）。协调者挂了，参与者等待超时，默认提交事务

如果参与者异常，协调者也异常，会造成其他参与者提交



TCC

两阶段补偿性方案

实现一个事务，需要定义三个API：预先占有资源，确认提交实际操作资源，取消占有=回滚。

如果后两个阶段执行一半失败了，记录日志，补偿处理，通知人工

在业务层方面分布式事务，最终一致性，不会一直持有锁，没操作完一个库，就释放锁



消息中间件实现（消息队列柔性事务）

本地事务+定时任务+消息队列+事件表

```sql
CREATE TABLE `tbl_order_event` (
  `id` int(16) NOT NULL,
  `order_type` varchar(32) DEFAULT NULL COMMENT '事件类型（支付表支付完成，订单表修改状态）',
  `process` varchar(32) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '事件环节（new,published,processed)',
  `content` varchar(255) DEFAULT NULL COMMENT '事件内容，保存事件发生时需要传递的数据',
  `create_time` datetime DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP,
  `update_time` datetime DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
```

![](D:\0_LeargingSummary\分布式问题\images\消息队列柔性事务.png)

1、2、3属于一个事务

4、5属于一个事务

6、7属于一个事务

8，9，10属于一个事务



**seata框架**

https://seata.io/zh-cn/docs/overview/what-is-seata.html

AT模式

整体机制

两阶段提交协议的演变：

- 一阶段：业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源。
- 二阶段：
  - 提交异步化，非常快速地完成。
  - 回滚通过一阶段的回滚日志进行反向补偿。

> #### 工作机制

以一个示例来说明整个 AT 分支的工作过程。

业务表：`product`

| Field | Type         | Key  |
| ----- | ------------ | ---- |
| id    | bigint(20)   | PRI  |
| name  | varchar(100) |      |
| since | varchar(100) |      |

AT 分支事务的业务逻辑：

```sql
update product set name = 'GTS' where name = 'TXC';
```

> #### 一阶段

过程：

1. 解析 SQL：得到 SQL 的类型（UPDATE），表（product），条件（where name = 'TXC'）等相关的信息。
2. 查询前镜像：根据解析得到的条件信息，生成查询语句，定位数据。

```sql
select id, name, since from product where name = 'TXC';
```

得到前镜像：

| id   | name | since |
| ---- | ---- | ----- |
| 1    | TXC  | 2014  |

1. 执行业务 SQL：更新这条记录的 name 为 'GTS'。
2. 查询后镜像：根据前镜像的结果，通过 **主键** 定位数据。

```sql
select id, name, since from product where id = 1`;
```

得到后镜像：

| id   | name | since |
| ---- | ---- | ----- |
| 1    | GTS  | 2014  |

1. 插入回滚日志：把前后镜像数据以及业务 SQL 相关的信息组成一条回滚日志记录，插入到 `UNDO_LOG` 表中。

```json
{
	"branchId": 641789253,
	"undoItems": [{
		"afterImage": {
			"rows": [{
				"fields": [{
					"name": "id",
					"type": 4,
					"value": 1
				}, {
					"name": "name",
					"type": 12,
					"value": "GTS"
				}, {
					"name": "since",
					"type": 12,
					"value": "2014"
				}]
			}],
			"tableName": "product"
		},
		"beforeImage": {
			"rows": [{
				"fields": [{
					"name": "id",
					"type": 4,
					"value": 1
				}, {
					"name": "name",
					"type": 12,
					"value": "TXC"
				}, {
					"name": "since",
					"type": 12,
					"value": "2014"
				}]
			}],
			"tableName": "product"
		},
		"sqlType": "UPDATE"
	}],
	"xid": "xid:xxx"
}
```

1. 提交前，向 TC 注册分支：申请 `product` 表中，主键值等于 1 的记录的 **全局锁** 。
2. 本地事务提交：业务数据的更新和前面步骤中生成的 UNDO LOG 一并提交。
3. 将本地事务提交的结果上报给 TC。

> ####  二阶段-回滚

1. 收到 TC 的分支回滚请求，开启一个本地事务，执行如下操作。
2. 通过 XID 和 Branch ID 查找到相应的 UNDO LOG 记录。
3. 数据校验：拿 UNDO LOG 中的后镜与当前数据进行比较，如果有不同，说明数据被当前全局事务之外的动作做了修改。这种情况，需要根据配置策略来做处理，详细的说明在另外的文档中介绍。
4. 根据 UNDO LOG 中的前镜像和业务 SQL 的相关信息生成并执行回滚的语句：

```sql
update product set name = 'TXC' where id = 1;
```

1. 提交本地事务。并把本地事务的执行结果（即分支事务回滚的结果）上报给 TC。

> #### 二阶段-提交

1. 收到 TC 的分支提交请求，把请求放入一个异步任务的队列中，马上返回提交成功的结果给 TC。
2. 异步任务阶段的分支提交请求将异步和批量地删除相应 UNDO LOG 记录。

![](D:\0_LeargingSummary\分布式问题\images\seata AT 模式.png)